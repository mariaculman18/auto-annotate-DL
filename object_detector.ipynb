{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c31121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# Automatic annotation using self-supervised learning and transformers\n",
    "# Written by Maria Culman\n",
    "# Based on:\n",
    "# DINO (https://arxiv.org/abs/2104.14294) \n",
    "# LOST (https://arxiv.org/abs/2109.14279)\n",
    "# ConvNeXt (https://arxiv.org/abs/2201.03545)\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "## Import generic libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "## Import specialized libraries\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "import timm\n",
    "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification, pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as pth_transforms\n",
    "from scipy.ndimage import label, find_objects\n",
    "import vision_transformer as vits\n",
    "import urllib.request\n",
    "\n",
    "# os.system(\"wget https://dl.fbaipublicfiles.com/convnext/label_to_words.json\")\n",
    "# imagenet_labels = json.load(open('label_to_words.json'))\n",
    "\n",
    "url = \"https://dl.fbaipublicfiles.com/convnext/label_to_words.json\"\n",
    "filename = 'label_to_words.json'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "imagenet_labels = json.load(open('label_to_words.json'))\n",
    "\n",
    "## Fuctions\n",
    "def dino(arch, patch_size, device): \n",
    "    \"\"\"\n",
    "    Creation of a model (ViT) from DINO to extract features that help discover objects.\n",
    "    Inputs\n",
    "        arch (str): name of the Vision Transformer trained with DINO to be implemented\n",
    "        patch_size (int): size of patches for the original to be devided. Options are 8 and 16 pixels. Smaller patches provided fine-grained discoveries.\n",
    "        device (str): computer device where the model should be stored. CPU is preferred for inference.\n",
    "    Outputs\n",
    "        model (obj): model loaded with pre-trained weights\n",
    "    \"\"\"\n",
    "    # Use auxiliary python file to create the model's achitecture\n",
    "    model = vits.__dict__[arch](patch_size = patch_size, num_classes = 0)\n",
    "    \n",
    "    # Freeze the model so it is used as pre-trained\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Initialize model with pre-trained weights on ImageNet-1k dataset\n",
    "    if patch_size == 16:\n",
    "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "    elif patch_size == 8:\n",
    "        url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "    state_dict = torch.hub.load_state_dict_from_url(\n",
    "                url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "    msg = model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(url, msg))\n",
    "\n",
    "    # Make model avilable for inference in accessible device\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def lost(feats, dims, scales, init_image_size, n_seeds, correl_th):\n",
    "    \"\"\"\n",
    "    Adaptation of LOST method to discover multiple objects.\n",
    "    Inputs\n",
    "        feats (tensor): the pixel/patche features of an image obtained from DINO\n",
    "        dims (list of int): dimension of the map from which the features are used\n",
    "        scales (list of int): from image to map scale\n",
    "        init_image_size (int): size of the image\n",
    "        n_seeds (int): number of potential seeds to discover objects from\n",
    "        correl_th (int): correlation threshold to add pixels to the seed to be considered one object\n",
    "    Outputs\n",
    "        preds (list of arrays): object discoveries\n",
    "        scores_scale (list of floats): correlation degree of object discoveries scaled to 0-1\n",
    "    \"\"\"\n",
    "    # Compute the similarity\n",
    "    A = (feats @ feats.transpose(1, 2)).squeeze()\n",
    "\n",
    "    # Compute the inverse degree centrality measure per patch\n",
    "    sorted_patches, scores = patch_scoring(A)\n",
    "\n",
    "    # Select the initial seeds\n",
    "    seeds = sorted_patches[:n_seeds]\n",
    "    scores_seeds = scores[:n_seeds]\n",
    "    \n",
    "    # Cluster seeds together and select less correlating seed per cluster\n",
    "    seeds_cluster, scores_cluster = clean_seeds(seeds, scores_seeds)\n",
    "    \n",
    "    # Scale correlating degree 0-1\n",
    "    max_score = dims[0] * dims[1]\n",
    "    scores_scale = (scores_cluster - (-max_score)) / (0 - (-max_score))\n",
    "    \n",
    "    # Create a discovery box around the seed considering the minimum correlation \n",
    "    preds = []\n",
    "    for s in seeds_cluster:\n",
    "        # Box extraction\n",
    "        pred, _ = detect_box(A[s, :], s, dims, correl_th, scales = scales, initial_im_size = init_image_size[1:], )\n",
    "        preds.append(np.asarray(pred))\n",
    "\n",
    "    return preds, scores_scale\n",
    "\n",
    "def patch_scoring(M, threshold=0.):\n",
    "    \"\"\"\n",
    "    Patch scoring based on the inverse degree.\n",
    "    \"\"\"\n",
    "    # Cloning important\n",
    "    A = M.clone()\n",
    "\n",
    "    # Zero diagonal\n",
    "    A.fill_diagonal_(0)\n",
    "\n",
    "    # Make sure symmetric and non nul\n",
    "    A[A < 0] = 0\n",
    "    C = A + A.t()\n",
    "\n",
    "    # Sort pixels by inverse degree\n",
    "    cent = -torch.sum(A > threshold, dim=1).type(torch.float32)\n",
    "    sort, sel = torch.sort(cent, descending=True)\n",
    "\n",
    "    return sel, sort\n",
    "\n",
    "def clean_seeds(seeds, scores):\n",
    "    \"\"\"\n",
    "    Cluster spatially adjacent seeds and leave the seed with lowest correlation degree per cluster.\n",
    "    \"\"\"\n",
    "    # Create auxiliary structures for clustering and scoring\n",
    "    seed_array = np.zeros((w_featmap, h_featmap))\n",
    "    scores_array = np.empty((w_featmap, h_featmap))\n",
    "    scores_array[:] = np.NaN\n",
    "    seed_array_2 = np.zeros((w_featmap, h_featmap))\n",
    "    \n",
    "    # Organize seeds per location \n",
    "    seeds_1, indices = torch.sort(seeds)\n",
    "    scores_1 = scores[indices]\n",
    "    \n",
    "    # Locate seeds in clustering structure\n",
    "    for i, s in enumerate(seeds_1):\n",
    "        center = np.unravel_index(s.cpu().numpy(), (w_featmap, h_featmap))\n",
    "        seed_array[center[0]][center[1]] = 1\n",
    "        scores_array[center[0]][center[1]] = scores_1[i].cpu().numpy()\n",
    "\n",
    "    # Cluster adjacent seeds \n",
    "    labeled_seed, num_features = label(seed_array)\n",
    "\n",
    "    # Leave one seed per cluster with the lowest correlation degree\n",
    "    seeds_2 = []\n",
    "    for n in range(num_features):\n",
    "        loc = find_objects(labeled_seed == (n + 1))[0]\n",
    "        n_labeled_array = labeled_seed[loc]\n",
    "        n_scores_array = scores_array[loc]\n",
    "\n",
    "        ind = np.unravel_index(np.nanargmax(n_scores_array, axis=None), n_scores_array.shape)\n",
    "\n",
    "        n_seeds_array_2 = np.zeros(n_labeled_array.shape)\n",
    "        n_seeds_array_2[ind] = n + 1\n",
    "\n",
    "        seed_array_2[loc] = n_seeds_array_2\n",
    "        seeds_ind = np.asarray(np.where(seed_array_2 == n + 1)).T\n",
    "\n",
    "        pos = np.ravel_multi_index(seeds_ind[0], (w_featmap, h_featmap))\n",
    "        seeds_2.append(pos)\n",
    "\n",
    "    # Resolve conflicting seeds in the case of two with the lowest degree\n",
    "    seeds_3 = np.unique(np.array(seeds_2))\n",
    "    seeds_final, indices = torch.sort(torch.as_tensor(seeds_3))\n",
    "    c = np.isin(seeds_1.cpu().numpy(), seeds_final)\n",
    "    scores_final = scores_1[c]\n",
    "\n",
    "    return seeds_final, scores_final\n",
    "\n",
    "def detect_box(A, seed, dims, correl_th, initial_im_size=None, scales=None):\n",
    "    \"\"\"\n",
    "    Extract a box corresponding to the seed patch.\n",
    "    \"\"\"\n",
    "    w_featmap, h_featmap = dims\n",
    "\n",
    "    correl = A.reshape(w_featmap, h_featmap).float()\n",
    "\n",
    "    # Compute connected components\n",
    "    labeled_array, num_features = label(correl.cpu().numpy() > correl_th)\n",
    "\n",
    "    # Find connected component corresponding to the initial seed\n",
    "    cc = labeled_array[np.unravel_index(seed.cpu().numpy(), (w_featmap, h_featmap))]\n",
    "    if cc == 0:\n",
    "        pred = [0, 0, 0, 0]\n",
    "        pred_feats = [0, 0, 0, 0]\n",
    "\n",
    "    else:\n",
    "        # Find box\n",
    "        mask = np.where(labeled_array == cc)\n",
    "        # Add +1 because excluded max\n",
    "        ymin, ymax = min(mask[0]), max(mask[0]) + 1\n",
    "        xmin, xmax = min(mask[1]), max(mask[1]) + 1\n",
    "\n",
    "        # Rescale to image size\n",
    "        r_xmin, r_xmax = scales[1] * xmin, scales[1] * xmax\n",
    "        r_ymin, r_ymax = scales[0] * ymin, scales[0] * ymax\n",
    "\n",
    "        pred = [r_xmin, r_ymin, r_xmax, r_ymax]\n",
    "\n",
    "        # Check not out of image size (used when padding)\n",
    "        if initial_im_size:\n",
    "            pred[2] = min(pred[2], initial_im_size[1])\n",
    "            pred[3] = min(pred[3], initial_im_size[0])\n",
    "\n",
    "        # Coordinate predictions for the feature space\n",
    "        # Axis different then in image space\n",
    "        pred_feats = [ymin, xmin, ymax, xmax]\n",
    "\n",
    "    return pred, pred_feats\n",
    "\n",
    "def convnext(inp, final_pred):\n",
    "    \"\"\"\n",
    "    Classify object discoveries. Among the predicted classes, select the one with highest confidence.\n",
    "    Inputs\n",
    "        inp (tensor and str): tensor of transformed image and image file path\n",
    "        final_pred (list of arrays): object discoveries cleaned by non-maximum suppression\n",
    "    Outputs\n",
    "        labels (list of str): object classes from detections with highest confidence\n",
    "        confidences (list of floats): object confidences from detections with highest confidence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open image \n",
    "    img_dis = Image.open(inp[1])\n",
    "    \n",
    "    # Create cropped images centered at box discovery and use Hugging Face space of ConvNeXt to retrieve image classification\n",
    "    labels = []\n",
    "    confidences = []\n",
    "    for i, p in enumerate(final_pred):\n",
    "        [bbox_x, bbox_y, bbox_w, bbox_h] = p[0], p[1], p[2]-p[0], p[3]-p[1]\n",
    "\n",
    "        # Crop image to standard size\n",
    "        x_center = bbox_x + (bbox_w / 2)\n",
    "        y_center = bbox_y + (bbox_h / 2)\n",
    "        s = 128\n",
    "        box_dis = img_dis.crop((x_center - (s / 2), y_center - (s / 2), x_center + (s / 2), y_center + (s / 2)))\n",
    "        \n",
    "        # Load image to ConvNeXt model and get classification results\n",
    "        model = timm.create_model('convnext_xlarge.fb_in22k', pretrained=True)\n",
    "        model = model.eval()\n",
    "        data_config = timm.data.resolve_model_data_config(model)\n",
    "        transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "        out = model(transforms(box_dis).unsqueeze(0))\n",
    "        \n",
    "        # Keep label and confidence of classification with highest confidence.\n",
    "        output = torch.topk(out.softmax(dim=1), k=1)\n",
    "        index = output.indices[0]\n",
    "        label = imagenet_labels[str(int(index))]\n",
    "        labels.append(label)\n",
    "        confidence = output.values[0].item()\n",
    "        confidences.append(confidence)\n",
    "        \n",
    "#         # Alternative ConvNext\n",
    "#         pipe = pipeline(\"image-classification\", \n",
    "#                 model=ConvNextForImageClassification.from_pretrained(\"facebook/convnext-xlarge-224-22k-1k\"),\n",
    "#                 feature_extractor=ConvNextFeatureExtractor.from_pretrained(\"facebook/convnext-xlarge-224-22k-1k\"))\n",
    "#         output = pipe(box_dis)\n",
    "#         # Keep label and confidence of classification with highest confidence.\n",
    "#         label = list(output[0].values())[1]\n",
    "#         labels.append(label)\n",
    "#         confidence = list(output[0].values())[0]\n",
    "#         confidences.append(confidence)\n",
    "    \n",
    "    return labels, confidences\n",
    "\n",
    "def visualize_predictions(image, preds, output_dir, im_name, number_seeds, correlation_th, labels, confidences):\n",
    "    \"\"\"\n",
    "    Visualization of the predicted objects (boxes, classes, and confidences).\n",
    "    \"\"\"\n",
    "    # Define colors for visualization\n",
    "    COLORS = [[0.000, 0.500, 0.850], [0.750, 0.425, 0.100], [0.950, 0.700, 0.150],\n",
    "          [0.500, 0.200, 0.600], [0.400, 0.650, 0.150], [0.300, 0.800, 0.950]]\n",
    "    colors = COLORS * 100\n",
    "   \n",
    "    # Open image in a plot\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    # Draw each prediction if its confidence is above 0.20    \n",
    "    for (xmin, ymin, xmax, ymax), label, conf, c in zip(preds, labels, confidences, colors):\n",
    "        label = label[:label.find(\",\")] if label.find(',') != -1 else label\n",
    "        if conf > 0.20:\n",
    "            w = xmax - xmin\n",
    "            h = ymax - ymin\n",
    "            ax.add_patch(plt.Rectangle((xmin, ymin), w, h,\n",
    "                                       fill = False, color = c, linewidth = 3))\n",
    "            text = f'{label}: {conf:.2f}'\n",
    "            ax.text(xmin, ymin, text, fontsize = 15,\n",
    "                    bbox = dict(facecolor = 'yellow', alpha = 0.5))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Remove axis from plot, and save and display plot\n",
    "    plt.axis('off')\n",
    "    pltname = f\"{output_dir}/pred_{im_name}_{number_seeds}_{correlation_th}.jpg\"\n",
    "    plt.savefig(pltname)\n",
    "    plt.show()\n",
    "\n",
    "## Classes \n",
    "class DatasetCustom:\n",
    "    \"\"\"\n",
    "    Class to instantiate a custom dataset that opens and handles images as necessary.\n",
    "    Images contained in the image folder are loaded as tensors.\n",
    "    Fuctions to retrieve the image name and load it are made available.\n",
    "    \"\"\"    \n",
    "    def __init__(self, image_folder):\n",
    "        self.image_dir = image_folder\n",
    "        images = [os.path.join(r, fn) for r, ds, fs in os.walk(self.image_dir) for fn in fs if fn.endswith('.jpg')]\n",
    "        images.sort()\n",
    "        \n",
    "        transform = pth_transforms.Compose(\n",
    "            [pth_transforms.ToTensor(), \n",
    "             pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), ])\n",
    "        \n",
    "        # Load tensor of transformed image and image file path \n",
    "        self.dataloader = [[transform((Image.open(im)).convert(\"RGB\")), im] for im in images]\n",
    "\n",
    "    def get_image_name(self, inp):\n",
    "        \"\"\"\n",
    "        Return the image name\n",
    "        \"\"\"\n",
    "        file_name = os.path.basename(inp)\n",
    "        im_name = \"{}\".format(file_name[:file_name.rfind(\".\")])\n",
    "\n",
    "        return im_name\n",
    "\n",
    "    def load_image(self, im_name):\n",
    "        \"\"\"\n",
    "        Load the image corresponding to the im_name\n",
    "        \"\"\"\n",
    "        im = os.path.join(self.image_dir, im_name + '.jpg')\n",
    "        image = io.imread(im)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main code\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Declare folder paths for input images and output results\n",
    "    image_dir = \"./images/\"\n",
    "    output_dir = \"./output/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Define object discovery parameters\n",
    "    patch_size = 8 # Options are 8 and 16 pixels\n",
    "    number_seeds = 40 # Number of potential objects to detect\n",
    "    correlation_th = 75 # Minimum correlation to extend an object discovery \n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Dataset\n",
    "    dataset = DatasetCustom(image_dir) # Creates dataset object\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Device and Model\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # Defines place to load model\n",
    "    model = dino(\"vit_base\", patch_size, device) # Creates and loads model from DINO\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------\n",
    "    # Loop over images to discover and classify objects\n",
    "    pbar = tqdm(dataset.dataloader)\n",
    "    for im_id, inp in enumerate(pbar):\n",
    "\n",
    "        # ------------ IMAGE PROCESSING ---------------------------------------------------------------------\n",
    "        img = inp[0]\n",
    "        init_image_size = img.shape\n",
    "\n",
    "        # Get the name of the image\n",
    "        im_name = dataset.get_image_name(inp[1])\n",
    "\n",
    "        # Padding the image with zeros to fit multiple of patch-size\n",
    "        size_im = (\n",
    "            img.shape[0],\n",
    "            int(np.ceil(img.shape[1] / patch_size) * patch_size),\n",
    "            int(np.ceil(img.shape[2] / patch_size) * patch_size),\n",
    "        )\n",
    "        paded = torch.zeros(size_im)\n",
    "        paded[:, : img.shape[1], : img.shape[2]] = img\n",
    "        img = paded.to(device)\n",
    "\n",
    "        # Move to gpu\n",
    "        # img = img.cuda(non_blocking=True) # Make line available if working in GPU device instead of CPU device\n",
    "        \n",
    "        # Size for transformers\n",
    "        w_featmap = img.shape[-2] // patch_size\n",
    "        h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "\n",
    "        # ------------ 1. Apply DINO to extract features -------------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # ------------ FORWARD PASS ---------------------------------------------------------------------\n",
    "            # Store the outputs of qkv layer from the last attention layer\n",
    "            feat_out = {}\n",
    "\n",
    "            def hook_fn_forward_qkv(module, input, output):\n",
    "                feat_out[\"qkv\"] = output\n",
    "\n",
    "            model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "\n",
    "            # Forward pass in the model\n",
    "            attentions = model.get_last_selfattention(img[None, :, :, :])\n",
    "\n",
    "            # Scaling factor\n",
    "            scales = [patch_size, patch_size]\n",
    "\n",
    "            # Dimensions\n",
    "            nb_im = attentions.shape[0]  # Batch size\n",
    "            nh = attentions.shape[1]  # Number of heads\n",
    "            nb_tokens = attentions.shape[2]  # Number of tokens\n",
    "\n",
    "            # Extract the qkv features of the last attention layer\n",
    "            qkv = (feat_out[\"qkv\"]\n",
    "                    .reshape(nb_im, nb_tokens, 3, nh, -1 // nh)\n",
    "                    .permute(2, 0, 3, 1, 4))\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            k = k.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
    "\n",
    "            # Select the keys extracted from the layer\n",
    "            feats = k[:, 1:, :]\n",
    "\n",
    "        # ------------ 2. Apply LOST for multiple-object discovery ---------------------------------------------\n",
    "        # Discover the potential objects located in the image\n",
    "        preds, scores = lost(\n",
    "            feats,\n",
    "            [w_featmap, h_featmap],\n",
    "            scales,\n",
    "            init_image_size,\n",
    "            number_seeds, correlation_th)\n",
    "\n",
    "        # Apply non-maximum suppression to eliminate overlapping discoveries\n",
    "        indices = cv2.dnn.NMSBoxes(preds, scores.cpu().numpy(), score_threshold=0.4, nms_threshold=0.5)\n",
    "        final_pred = np.array(preds)[indices]\n",
    "        final_scores = np.array(scores.cpu().numpy())[indices]\n",
    "        \n",
    "        # ------------ 3. Apply ConvNeXt to classify object discoveries ---------------------------------------- \n",
    "        # Assign the most probable class to each of the object discoveries\n",
    "        labels, confidences = convnext(inp, final_pred)\n",
    "        print(f'\\nPredictions for image {im_name}:')\n",
    "        mapped = zip(labels, confidences)\n",
    "        print(set(mapped))\n",
    "\n",
    "        # ------------ 4. Visualize image with detection results -----------------------------------------------\n",
    "        image = dataset.load_image(im_name)\n",
    "        visualize_predictions(image, final_pred, output_dir, im_name, number_seeds, correlation_th, labels, confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91dd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (object_det_env)",
   "language": "python",
   "name": "object_det_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
